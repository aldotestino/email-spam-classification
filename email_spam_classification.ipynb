{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIltjcy77SEB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OybRA32_6I5p"
      },
      "outputs": [],
      "source": [
        "data_file1_path = '/content/drive/MyDrive/Colab Notebooks/Statistical Methods/spam_ham_dataset.csv'\n",
        "data_file2_path = '/content/drive/MyDrive/Colab Notebooks/Statistical Methods/SMSSpamCollection.csv'\n",
        "\n",
        "paths = [data_file1_path, data_file2_path]\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for path in paths:\n",
        "  df = pd.read_csv(path)\n",
        "  texts += df['text'].tolist()\n",
        "  labels += [1 if label == 'spam' else 0 for label in df['label'].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O97F0lwwKBIn"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aaA6ZBRNeJj"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "    encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'label': torch.tensor(label)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD0EOCHWNodS"
      },
      "outputs": [],
      "source": [
        "bert_model_name = 'bert-base-cased'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "learning_rate = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlgKxflvLKhS"
      },
      "outputs": [],
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAZh4L7tO8Oq"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert_model_name, num_classes):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    x = self.dropout(pooled_output)\n",
        "    logits = self.fc(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fD2T-j1Owh3"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4HD6CoJQnJH"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "  model.train()\n",
        "  for batch in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actual_labels = []\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader():\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds.cpu().tolist())\n",
        "      actual_labels.extend(labels.cpu()).tolist()\n",
        "\n",
        "  return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXyZz6OOSmBX"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "  train(model, train_dataloader, optimizer, scheduler, device)\n",
        "  accuracy, report = evaluate(model, val_dataloader, device)\n",
        "  print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "  print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqeeDAMBS_Qv"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'bert_classifier.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ei_Kn3TWRsn"
      },
      "source": [
        "# Inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyvTH4drWRXM"
      },
      "outputs": [],
      "source": [
        "def predict_ham_spam(text, model, tokenizer, device, max_length=128):\n",
        "  model.eval()\n",
        "  encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return \"spam\" if preds.item() == 1 else \"ham\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTaIAiAYWljH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "model.load_state_dict(torch.load('bert_classifier.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_VNftsQXMRm"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    \"Had your mobile 11 months or more? You are entitled to update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\",\n",
        "    \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\"\n",
        "]\n",
        "\n",
        "for test_text in test_texts:\n",
        "  prediction = predict_ham_spam(test_text, model, tokenizer, device)\n",
        "  print(test_text)\n",
        "  print(f'Prediction: {prediction}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
